import logging
import json
import re
from autogen import ConversableAgent
# ä» agentchat å­æ¨¡å—å¯¼å…¥
from autogen.agentchat import initiate_group_chat
from autogen.agentchat.group.patterns import RoundRobinPattern
from autogen.agentchat.groupchat import GroupChatManager

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å‹å®¢æˆ·ç«¯æ‰€éœ€çš„åº“
from types import SimpleNamespace
from typing import List, Dict, Union
from llama_cpp import Llama 

# -----------------------------
# å¯¼å…¥ StrategyPlanner ç›¸å…³ä¾èµ–
# -----------------------------
from config import StrategyPlannerConfig
from schema import StrategyRequest, FallacySignal
from strategy_planner import StrategyPlanner

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==============================================================================
# æ ¸å¿ƒä¿®å¤ 1: åŸºäºæ ˆçš„é²æ£’ JSON æå–å™¨
# ==============================================================================
def extract_json_with_stack(text: str) -> dict:
    """
    ä½¿ç”¨å †æ ˆé€»è¾‘ä»æ–‡æœ¬ä¸­æå–æœ€å¤–å±‚çš„ JSON å¯¹è±¡ã€‚
    è¿™æ¯”æ­£åˆ™è¡¨è¾¾å¼æ›´å¯é ï¼Œèƒ½å¤„ç†åµŒå¥—çš„å¤§æ‹¬å·ã€‚
    """
    text = text.strip()
    
    # å°è¯•å¯»æ‰¾ç¬¬ä¸€ä¸ª {
    start_idx = text.find('{')
    if start_idx == -1:
        raise ValueError("No '{' found")
    
    stack = []
    json_str = ""
    
    # ä»ç¬¬ä¸€ä¸ª { å¼€å§‹éå†
    for i in range(start_idx, len(text)):
        char = text[i]
        if char == '{':
            stack.append('{')
        elif char == '}':
            if stack:
                stack.pop()
        
        # è®°å½•å½“å‰å­—ç¬¦
        json_str += char
        
        # å¦‚æœæ ˆç©ºäº†ï¼Œè¯´æ˜æ‰¾åˆ°äº†é—­åˆçš„æœ€å¤–å±‚å¯¹è±¡
        if not stack:
            try:
                # å°è¯•è§£ææå–åˆ°çš„ç‰‡æ®µ
                return json.loads(json_str)
            except json.JSONDecodeError:
                # å¦‚æœè§£æå¤±è´¥ï¼ˆæ¯”å¦‚å†…éƒ¨æœ‰è¯­æ³•é”™è¯¯ï¼‰ï¼Œç»§ç»­å°è¯•æ‰¾ä¸‹ä¸€ä¸ªé—­åˆ
                raise ValueError("Found matching braces but content is invalid JSON")
                
    raise ValueError("Unbalanced braces or invalid JSON")

# ---------------------------------------------
# 1. å®šä¹‰ Llama.cpp Custom Model Client
# ---------------------------------------------
class LlamaCppClient:
    """éµå¾ª Autogen ModelClient åè®®çš„è‡ªå®šä¹‰å®¢æˆ·ç«¯"""
    RESPONSE_USAGE_KEYS = ["prompt_tokens", "completion_tokens", "total_tokens", "cost", "model"]

    def __init__(self, config: Dict, **kwargs):
        model_path = config.get("model_path")
        self.model_name = config.get("model", "llama-3-8b-local")
        self.temperature = config.get("temperature", 0.7)
        self.max_tokens = config.get("max_tokens", 512)
        
        self.llama = Llama(
            model_path=model_path, 
            n_ctx=self.max_tokens * 4, # åŠ å¤§ä¸Šä¸‹æ–‡çª—å£é˜²æ­¢æˆªæ–­
            n_gpu_layers=-1,
            verbose=False
        )
        print(f"âœ… LlamaCppClient initialized with model: {model_path}")

    def create(self, params: Dict) -> SimpleNamespace:
        messages = params.get("messages", [])
        
        # æ„å»º Prompt
        prompt = self._messages_to_prompt(messages)
        
        try:
            # ç»Ÿä¸€ä½¿ç”¨ completion æ¥å£ä»¥è·å¾—æ›´ç¨³å®šçš„æ§åˆ¶
            response_data = self.llama.create_completion(
                prompt=prompt,
                temperature=self.temperature,
                max_tokens=params.get("max_tokens", self.max_tokens),
                stop=params.get("stop", ["<|eot_id|>"]), # ç¡®ä¿åŠæ—¶åœæ­¢
            )
        except Exception as e:
            print(f"Llama inference failed: {e}")
            return SimpleNamespace(choices=[], model=self.model_name, usage={})

        # å°è£…å“åº”
        response = SimpleNamespace()
        response.choices = []
        response.model = self.model_name
        
        content = response_data['choices'][0]['text']
        choice = SimpleNamespace(message=SimpleNamespace(content=content, role='assistant'))
        response.choices.append(choice)
        response.usage = response_data.get('usage', {})
        return response

    def _messages_to_prompt(self, messages: List[Dict]) -> str:
        """Llama 3 æ ‡å‡† Prompt æ ¼å¼"""
        prompt = ""
        for message in messages:
            role = message.get("role")
            content = message.get("content", "")
            if role == "system":
                prompt += f"<|start_header_id|>system<|end_header_id|>\n\n{content}<|eot_id|>"
            elif role == "user":
                prompt += f"<|start_header_id|>user<|end_header_id|>\n\n{content}<|eot_id|>"
            elif role == "assistant":
                prompt += f"<|start_header_id|>assistant<|end_header_id|>\n\n{content}<|eot_id|>"
        prompt += "<|start_header_id|>assistant<|end_header_id|>\n\n"
        return prompt

    def message_retrieval(self, response: SimpleNamespace) -> Union[List[str], List[SimpleNamespace]]:
        return [choice.message for choice in response.choices]

    def cost(self, response: SimpleNamespace) -> float:
        return 0.0

    @staticmethod
    def get_usage(response: SimpleNamespace) -> Dict:
        return {}

# ---------------------------------------------
# 1.1 æ ¸å¿ƒä¿®å¤ 2: å¼ºå£®çš„é€‚é…å™¨
# ---------------------------------------------
class LocalLLMAdapterForPlanner:
    def __init__(self, autogen_client: LlamaCppClient):
        self.client = autogen_client

    def create_completion(self, messages, **kwargs):
        params = {
            "messages": messages,
            "max_tokens": kwargs.get("max_tokens", 800), # é™åˆ¶ç”Ÿæˆé•¿åº¦ï¼Œé™ä½é”™è¯¯ç‡
            "temperature": kwargs.get("temperature", 0.6)
        }
        response = self.client.create(params)
        raw_content = response.choices[0].message.content
        
        # --- è¿™é‡Œçš„é€»è¾‘ä¿è¯äº† StrategyPlanner ç»å¯¹ä¸ä¼šå› ä¸º JSON æ ¼å¼è€Œå´©æºƒ ---
        try:
            # 1. å°è¯•ä½¿ç”¨æ ˆæå–å™¨æ¸…æ´—æ•°æ®
            json_obj = extract_json_with_stack(raw_content)
            # æˆåŠŸæ¸…æ´—ï¼Œé‡æ–°æ‰“åŒ…æˆæ ‡å‡†å­—ç¬¦ä¸²
            return json.dumps(json_obj, ensure_ascii=False)
        except Exception as e:
            logger.warning(f"JSON æå–å¤±è´¥ ({e})ï¼Œæ­£åœ¨ä½¿ç”¨å…œåº•ç­–ç•¥ã€‚åŸå§‹è¾“å‡ºç‰‡æ®µ: {raw_content[:50]}...")
            
            # 2. æ„é€ å…œåº• JSON (å¿…é¡»åŒ…å« StrategyPlanner éœ€è¦çš„æ‰€æœ‰å­—æ®µ)
            # StrategyPlanner é€šå¸¸éœ€è¦ 'plan', 'analysis', 'scores', 'rationale'
            fallback_json = {
                "plan": f"ï¼ˆç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆï¼‰ç”±äºæ¨¡å‹è¾“å‡ºæ ¼å¼å¼‚å¸¸ï¼Œæ— æ³•ç”Ÿæˆé’ˆå¯¹æ€§ç­–ç•¥ã€‚å»ºè®®ç›´æ¥æŒ‡å‡ºå¯¹æ–¹çš„é€»è¾‘æ¼æ´ï¼š{messages[-1].get('content', '')[-50:]}...",
                "analysis": "æ¨¡å‹è¾“å‡ºæ— æ³•è§£æä¸ºæ ‡å‡† JSONã€‚",
                "scores": {"feasibility": 5, "effectiveness": 5},
                "rationale": "JSON Parsing Error Fallback"
            }
            return json.dumps(fallback_json, ensure_ascii=False)

    def chat(self, messages, **kwargs):
        return self.create_completion(messages, **kwargs)

# ---------------------------------------------
# 2. Autogen é…ç½®
# ---------------------------------------------
llm_config = {
    "temperature": 0.7,
    "config_list": [
        {
            "model": "llama-3-8b-local", 
            "model_client_cls": "LlamaCppClient",
            "model_path": "C:/Users/xing5/Downloads/Meta-Llama-3-8B-Instruct.Q4_0.gguf",
            "max_tokens": 1024,
        }
    ],
}

# å®ä¾‹åŒ– Client å’Œ Adapter
llama_client_instance = LlamaCppClient(config=llm_config["config_list"][0])
planner_adapter = LocalLLMAdapterForPlanner(llama_client_instance)

# -----------------------------
# 3. å®šä¹‰ Agent
# -----------------------------

analyzer = ConversableAgent(
    name="argument_analyzer",
    system_message="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è®ºè¯åˆ†æå¸ˆã€‚è¯·æå–ç”¨æˆ·è®ºç‚¹ã€å‰æå’Œæ¨ç†ç»“æ„ã€‚**è¯·åŠ¡å¿…å…¨ç¨‹ä½¿ç”¨ä¸­æ–‡å›ç­”**ã€‚",
    llm_config=llm_config,
)

critic_system_prompt = """
ä½ æ˜¯ä¸€ä¸ªé€»è¾‘æ‰¹åˆ¤ä¸“å®¶ã€‚ä½ è´Ÿè´£æŒ‡å‡ºé€»è¾‘è°¬è¯¯ã€‚
**å¿…é¡»**ä¸”**åªèƒ½**è¾“å‡º JSON æ ¼å¼ã€‚ä¸è¦è¾“å‡ºä»»ä½•å¼€å¤´æˆ–ç»“å°¾çš„åºŸè¯ã€‚
æ ¼å¼ï¼š
{
    "fallacy_type": "è°¬è¯¯åç§°",
    "confidence": 0.9,
    "reasoning": "ç®€çŸ­ç†ç”±"
}
"""

critic = ConversableAgent(
    name="logic_critic",
    system_message=critic_system_prompt,
    llm_config=llm_config,
)

# 3.3 Strategy Planner Agent
planner_agent = ConversableAgent(
    name="strategy_planner",
    system_message="è´Ÿè´£ç”Ÿæˆåé©³ç­–ç•¥ã€‚",
    llm_config=False, 
)

# åˆå§‹åŒ– Planner
planner_config = StrategyPlannerConfig(
    backend="local", 
    api_key="dummy", 
    model_name="llama-3",
    verbose=True 
)
strategy_logic = StrategyPlanner(planner_config, planner_adapter)

def planner_reply_func(recipient, messages, sender, config):
    last_message = messages[-1].get("content", "")
    print(f"\n[Debug] Critic Output: {last_message[:100]}...\n")

    # 1. è§£æ Critic JSON
    try:
        critic_data = extract_json_with_stack(last_message)
        fallacy_type = critic_data.get("fallacy_type", "é€šç”¨è°¬è¯¯")
        confidence = float(critic_data.get("confidence", 0.5))
    except Exception:
        fallacy_type = "é€šç”¨é€»è¾‘æ¼æ´"
        confidence = 0.5

    # 2. æ„é€ è¯·æ±‚
    # å°è¯•è·å– analyzer çš„å†…å®¹ä½œä¸ºåŸæ–‡
    opponent_text = "å¯¹æ–¹ä½¿ç”¨äº†é”™è¯¯çš„é€»è¾‘ã€‚" 
    for msg in reversed(messages):
        if msg.get("name") == "argument_analyzer":
            opponent_text = msg.get("content")
            break
            
    req = StrategyRequest(
        text=opponent_text,
        fallacy=FallacySignal(fallacy_type=fallacy_type, confidence=confidence),
        user_goal="win_debate",
        context=str([m['content'] for m in messages[-3:]]) 
    )

    # 3. è°ƒç”¨ Planner (ç°åœ¨å®ƒæ˜¯å®‰å…¨çš„)
    print(f"ğŸ”„ StrategyPlanner æ­£åœ¨ç”Ÿæˆç­–ç•¥... (ç±»å‹: {fallacy_type})")
    try:
        plan_result = strategy_logic.plan(req)
        
        # å®‰å…¨è·å–å±æ€§
        best_plan = getattr(plan_result, "plan", "æ— æ³•ç”Ÿæˆç­–ç•¥")
        rationale = getattr(plan_result, "rationale", "")
        
        reply_text = (
            f"ã€æ£€æµ‹è°¬è¯¯ã€‘{fallacy_type} (ç½®ä¿¡åº¦ {confidence})\n"
            f"ã€ç­–ç•¥å»ºè®®ã€‘\n{best_plan}\n"
            f"ã€ç†ç”±ã€‘\n{rationale}"
        )
        return True, reply_text
    except Exception as e:
        # è¿™å‡ ä¹ä¸å¯èƒ½å‘ç”Ÿäº†ï¼Œå› ä¸º adapter å·²ç»å…œåº•äº†
        return True, f"ç­–ç•¥ç”Ÿæˆå¼‚å¸¸: {str(e)}"

planner_agent.register_reply([ConversableAgent, None], planner_reply_func)

# 3.4 Generator
generator = ConversableAgent(
    name="counter_generator",
    system_message="ä½ è´Ÿè´£æ ¹æ®å‰ä¸€ä½ StrategyPlanner çš„ç­–ç•¥å»ºè®®ï¼Œç”Ÿæˆä¸€æ®µçŠ€åˆ©çš„ä¸­æ–‡åé©³ã€‚è¾“å‡ºåè¯·å•ç‹¬ä¸€è¡Œå†™ 'TERMINATE'ã€‚",
    is_termination_msg=lambda x:"TERMINATE" in (x.get("content","") or "").upper(),
    llm_config=llm_config,
)



claim = """
äººç±»ä¸éœ€è¦å‡å°‘ç¢³æ’æ”¾ï¼Œå› ä¸ºå‡ ç™¾å¹´æ¥åœ°çƒä¸€ç›´ä¼šè‡ªå·±è°ƒèŠ‚æ°”å€™ã€‚
å³ä½¿æˆ‘ä»¬ä»€ä¹ˆéƒ½ä¸åšï¼Œæ°”å€™å˜åŒ–æœ€ç»ˆä¹Ÿä¼šè‡ªå·±ç¨³å®šä¸‹æ¥ã€‚
æ‰€ä»¥æ‰€æœ‰åº”å¯¹æ°”å€™å˜åŒ–çš„æ”¿ç­–éƒ½æ˜¯æµªè´¹é’±ã€‚
"""


# -----------------------------
# 4. æ³¨å†Œä¸è¿è¡Œ
# -----------------------------
def register_custom_client(agent):
    agent.register_model_client(model_client_cls=LlamaCppClient)

register_custom_client(analyzer)
register_custom_client(critic)
register_custom_client(generator)

# 1. å…ˆåˆ›å»º GroupChat (åŒ…å«æ‰€æœ‰ agent)
# æ³¨æ„ï¼šä½ éœ€è¦æ‰‹åŠ¨å¯¼å…¥ GroupChat
from autogen.agentchat.groupchat import GroupChat 

group_chat = GroupChat(
    agents=[analyzer, critic, planner_agent, generator],
    messages=[],
    max_round=10,
    speaker_selection_method="round_robin" # æ˜¾å¼æŒ‡å®šè½®è¯¢ï¼Œæ›¿ä»£ AutoPattern
)

# 2. å†åˆ›å»º GroupChatManager (ç›´æ¥ä¼ å…¥ group_chat)
debate_manager = GroupChatManager(
    name="debate_manager",
    groupchat=group_chat, # åœ¨è¿™é‡Œç›´æ¥ä¼ å…¥ï¼ä¸è¦ç”¨ = None
    llm_config=False,     # ç¦ç”¨ Manager çš„ LLM
)

# 3. å¼€å§‹è¿è¡Œ
print("ğŸš€ å¼€å§‹è¿è¡Œå¤š Agent è¾©è®º (é›†æˆ StrategyPlanner)...")

debate_manager.initiate_chat(
    recipient=generator, # å¯¹äº GroupChatManagerï¼Œrecipient é€šå¸¸å¡«ç¾¤é‡Œçš„ä»»æ„ä¸€ä¸ª agent å³å¯
    message=claim,
)